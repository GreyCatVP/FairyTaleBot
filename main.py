
import os
import httpx
import asyncio
import re
import sys
from telegram import Update
from telegram.ext import ApplicationBuilder, CommandHandler, ContextTypes
from dotenv import load_dotenv

load_dotenv()

BOT_TOKEN = os.getenv("BOT_TOKEN")
OPENROUTER_API_KEY = os.getenv("OPENROUTER_API_KEY")
MODEL = os.getenv("OPENROUTER_MODEL", "deepseek/deepseek-r1-0528:free")

HEADERS = {
    "Authorization": f"Bearer {OPENROUTER_API_KEY}",
    "HTTP-Referer": "https://t.me/SkazochnikTimoshaBot",
    "Content-Type": "application/json"
}

async def gpt_call(messages, max_tokens=2048, retries=2):
    payload = {
        "model": MODEL,
        "messages": messages,
        "temperature": 0.8,
        "max_tokens": max_tokens
    }
    for attempt in range(retries + 1):
        try:
            sys.stderr.write(f"[üåÄ] –ü–æ–ø—ã—Ç–∫–∞ –∑–∞–ø—Ä–æ—Å–∞ #{attempt+1} –∫ GPT\n")
            async with httpx.AsyncClient(timeout=120.0) as client:
                r = await client.post("https://openrouter.ai/api/v1/chat/completions", headers=HEADERS, json=payload)
                r.raise_for_status()
                content = r.json()["choices"][0]["message"]["content"].strip()
                sys.stderr.write(f"[DEBUG] GPT –æ—Ç–≤–µ—Ç (–æ–±—Ä–µ–∑–∞–Ω–æ): {content[:100]}...\n")
                if any(msg in content.lower() for msg in ["—Å–ª–∏—à–∫–æ–º –º–Ω–æ–≥–æ –∑–∞–ø—Ä–æ—Å–æ–≤", "too many requests", "rate limit", "model is busy"]):
                    sys.stderr.write("[‚ö†Ô∏è] –ó–∞–≥–ª—É—à–∫–∞ –æ—Ç OpenRouter: GPT –Ω–µ –±—ã–ª –∑–∞–ø—É—â–µ–Ω.\n")
                    return "–°–µ–≥–æ–¥–Ω—è –≤–æ–ª—à–µ–±–Ω—ã–π –ø–æ—Ä—Ç–∞–ª –ø–µ—Ä–µ–≥—Ä—É–∂–µ–Ω. –Ø —Ç–∏—Ö–æ–Ω—å–∫–æ –ø–æ—Å–∏–∂—É –∏ –ø–æ–¥–æ–∂–¥—É. –ó–∞–≥–ª—è–Ω–∏ —á—É—Ç—å –ø–æ–∑–∂–µ ‚òÅÔ∏è"
                return content
        except httpx.HTTPStatusError as e:
            if e.response.status_code == 429:
                sys.stderr.write(f"[üö´] 429 Too Many Requests –æ—Ç OpenRouter (–ø–æ–ø—ã—Ç–∫–∞ {attempt+1})\n")
                if attempt < retries:
                    await asyncio.sleep(5)
                    continue
                return "–°–ª–∏—à–∫–æ–º –º–Ω–æ–≥–æ –∑–∞–ø—Ä–æ—Å–æ–≤. –ü–æ–¥–æ–∂–¥–∏ –Ω–µ–º–Ω–æ–≥–æ –∏ –ø–æ–ø—Ä–æ–±—É–π –µ—â—ë —Ä–∞–∑ ‚òï"
            raise
        except Exception as ex:
            sys.stderr.write(f"[‚ùå] –û—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±—Ä–∞—â–µ–Ω–∏–∏ –∫ GPT: {str(ex)}\n")
            return "–ü—Ä–æ–∏–∑–æ—à–ª–∞ –æ—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±—Ä–∞—â–µ–Ω–∏–∏ –∫ —Å–∫–∞–∑–æ—á–Ω–æ–º—É –ø–æ—Ä—Ç–∞–ª—É üò¢"

def is_truncated(text):
    last_word = re.split(r'\s+', text.strip())[-1]
    return not re.match(r'^[–ê-–Ø–∞-—èA-Za-z0-9—ë–Å\-‚Äì‚Äî"‚Äú‚Äù¬´¬ª!?.,;:‚Ä¶]+$', last_word)

def is_only_moral(text):
    lowered = text.lower()
    return (
        lowered.startswith("–≤–æ—Ç –∏ —Å–∫–∞–∑–∫–µ –∫–æ–Ω–µ—Ü") or
        "–º–æ—Ä–∞–ª—å" in lowered[:300] and len(text.split()) < 100
    )

async def generate_fairytale():
    base_prompt = [
        {"role": "system", "content": (
            "–¢—ã ‚Äî —Å–∫–∞–∑–æ—á–Ω–∏–∫. –ü–∏—à–∏ –¥–æ–±—Ä—ã–µ, –ø–æ–Ω—è—Ç–Ω—ã–µ —Å–∫–∞–∑–∫–∏ –¥–ª—è –¥–µ—Ç–µ–π 3‚Äì8 –ª–µ—Ç (—á–∏—Ç–∞—é—Ç —Ä–æ–¥–∏—Ç–µ–ª–∏). "
            "–ì–ª–∞–≤–Ω–æ–µ ‚Äî —Å—é–∂–µ—Ç, —ç–º–æ—Ü–∏–∏, —Ñ–∏–Ω–∞–ª —Å –º–æ—Ä–∞–ª—å—é. –ò–∑–±–µ–≥–∞–π –¥–ª–∏–Ω–Ω—ã—Ö –æ–ø–∏—Å–∞–Ω–∏–π –∏ –∏–∑–±—ã—Ç–æ—á–Ω—ã—Ö –º–µ—Ç–∞—Ñ–æ—Ä. "
            "–ü–∏—à–∏ –ª–∞–∫–æ–Ω–∏—á–Ω–æ, –æ–±—Ä–∞–∑–Ω–æ, –Ω–æ –ø—Ä–æ—Å—Ç–æ. –û–±—ä—ë–º ‚Äî 1500‚Äì2000 —Å–∏–º–≤–æ–ª–æ–≤."
        )},
        {"role": "user", "content": "–†–∞—Å—Å–∫–∞–∂–∏ –¥–æ–±—Ä—É—é —Å–∫–∞–∑–∫—É."}
    ]

    story = await gpt_call(base_prompt, retries=2)
    if "–≤–æ–ª—à–µ–±–Ω—ã–π –ø–æ—Ä—Ç–∞–ª –ø–µ—Ä–µ–≥—Ä—É–∂–µ–Ω" in story or "–°–ª–∏—à–∫–æ–º –º–Ω–æ–≥–æ –∑–∞–ø—Ä–æ—Å–æ–≤" in story:
        return story

    if is_only_moral(story):
        sys.stderr.write("[‚ùó] –ü–æ–ª—É—á–µ–Ω–∞ –º–æ—Ä–∞–ª—å –±–µ–∑ —Å–∫–∞–∑–∫–∏ ‚Äî –ø–æ–≤—Ç–æ—Ä–Ω–∞—è –≥–µ–Ω–µ—Ä–∞—Ü–∏—è\n")
        story = await gpt_call(base_prompt)

    verify_prompt = [{"role": "user", "content": f"–í–æ—Ç —Å–∫–∞–∑–∫–∞:\n\n{story}\n\n–°–∫–∞–∂–∏ —á–µ—Å—Ç–Ω–æ, –∑–∞–≤–µ—Ä—à–µ–Ω–∞ –ª–∏ –æ–Ω–∞ –ª–æ–≥–∏—á–µ—Å–∫–∏ –∏ —Ö—É–¥–æ–∂–µ—Å—Ç–≤–µ–Ω–Ω–æ? –û—Ç–≤–µ—Ç—å —Ç–æ–ª—å–∫–æ '–¥–∞' –∏–ª–∏ '–Ω–µ—Ç'."}]
    verdict = await gpt_call(verify_prompt, max_tokens=10)
    if not verdict.strip().lower().startswith("–¥–∞"):
        cont_prompt = base_prompt + [{"role": "assistant", "content": story}, {"role": "user", "content": "–ü–æ–∂–∞–ª—É–π—Å—Ç–∞, –ø—Ä–æ–¥–æ–ª–∂–∏ —Å–∫–∞–∑–∫—É –¥–æ –µ—ë –ø–æ–ª–Ω–æ–≥–æ –∑–∞–≤–µ—Ä—à–µ–Ω–∏—è —Å —Ñ–∏–Ω–∞–ª–æ–º –∏ –º–æ—Ä–∞–ª—å—é."}]
        continuation = await gpt_call(cont_prompt)
        story += "\n" + continuation

    last_line = story.strip().split("\n")[-1].lower()
    if any(last_line.endswith(w) for w in ["–ª–∞", "–µ–ª", "–ª—Å—è", "–∞–ª–∞", "–ª–∏", "–≤—Å—ë", "–Ω–µ—Ç"]) or len(last_line.split()) < 5 or is_truncated(story):
        final_prompt = base_prompt + [{"role": "assistant", "content": story}, {"role": "user", "content": "–ü–æ–∂–∞–ª—É–π—Å—Ç–∞, –¥–æ–ø–∏—à–∏ —Ñ–∏–Ω–∞–ª—å–Ω—É—é —Å—Ü–µ–Ω—É. –ì–µ—Ä–æ–∏ —É–∂–µ —Å–æ–±—Ä–∞–ª–∏—Å—å. –î–æ–±–∞–≤—å, –∫–∞–∫ –æ–Ω–∏ —á—É–≤—Å—Ç–≤—É—é—Ç —Å–µ–±—è, —á—Ç–æ –ø–æ–Ω—è–ª–∏, –∏ –∫–∞–∫–æ–π –≤—ã–≤–æ–¥ –æ—Å—Ç–∞–ª—Å—è."}]
        final_piece = await gpt_call(final_prompt)
        story += "\n" + final_piece

    if not story.strip().endswith("–í–æ—Ç –∏ —Å–∫–∞–∑–∫–µ –∫–æ–Ω–µ—Ü. ‚ú®"):
        story += "\n\n–í–æ—Ç –∏ —Å–∫–∞–∑–∫–µ –∫–æ–Ω–µ—Ü. ‚ú®"

    return story

def split_story(text, max_length=4096):
    parts = []
    paragraphs = text.split("\n")
    current = ""
    for p in paragraphs:
        if len(current) + len(p) + 1 > max_length:
            parts.append(current)
            current = p
        else:
            current += "\n" + p
    if current:
        parts.append(current)
    return parts

async def start(update: Update, context: ContextTypes.DEFAULT_TYPE):
    await update.message.reply_text("–ü—Ä–∏–≤–µ—Ç! –Ø ‚Äî –°–∫–∞–∑–æ—á–Ω–∏–∫ –¢–∏–º–æ—à–∞. ‚ú®\n–ù–∞–ø–∏—à–∏ /skazka ‚Äî –∏ —è —Ä–∞—Å—Å–∫–∞–∂—É —Ç–µ–±–µ –¥–æ–±—Ä—É—é —Å–∫–∞–∑–∫—É.")

async def skazka(update: Update, context: ContextTypes.DEFAULT_TYPE):
    await update.message.reply_text("–ü–æ–≥–æ–¥–∏ –Ω–µ–º–Ω–æ–≥–æ... —è –≤—Å–ø–æ–º–∏–Ω–∞—é —Å–∫–∞–∑–∫—É... ‚òï")
    story = await generate_fairytale()
    parts = split_story(story)
    for i, part in enumerate(parts):
        header = f"üìò –ß–∞—Å—Ç—å {i+1}/{len(parts)}\n" if len(parts) > 1 else ""
        await update.message.reply_text(header + part.strip())

if __name__ == "__main__":
    app = ApplicationBuilder().token(BOT_TOKEN).build()
    app.add_handler(CommandHandler("start", start))
    app.add_handler(CommandHandler("skazka", skazka))

    import threading
    from http.server import BaseHTTPRequestHandler, HTTPServer

    class DummyHandler(BaseHTTPRequestHandler):
        def do_GET(self):
            self.send_response(200)
            self.end_headers()
            self.wfile.write(b'Service is up')
        def do_HEAD(self):
            self.send_response(200)
            self.end_headers()

    def run_dummy_server():
        server = HTTPServer(('0.0.0.0', 8080), DummyHandler)
        server.serve_forever()

    threading.Thread(target=run_dummy_server, daemon=True).start()
    app.run_polling()
